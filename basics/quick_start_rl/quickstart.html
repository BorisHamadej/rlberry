

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
  <title>Quick Start for Reinforcement Learning in rlberry &mdash; rlberry v0.6.0.post25.dev0+7b18221  documentation</title>
  
  <link rel="canonical" href="https://rlberry-py.github.io/rlberry/basics/quick_start_rl/quickstart.html" />

  

  <link rel="stylesheet" href="../../_static/css/vendor/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
<script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
<script src="../../_static/jquery.js"></script> 
</head>
<body>


<nav id="navbar" class="sk-docs-navbar navbar navbar-expand-md navbar-light bg-light py-0">
  <div class="container-fluid sk-docs-container px-0">
    <button
      id="sk-navbar-toggler"
      class="navbar-toggler"
      type="button"
      data-toggle="collapse"
      data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent"
      aria-expanded="false"
      aria-label="Toggle navigation"
    >
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="sk-navbar-collapse collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mr-auto">
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../installation.html">Install</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../user_guide.html">User Guide</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../auto_examples/index.html">Examples</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../api.html">API</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../changelog.html">Changelog</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../about.html">about</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="https://github.com/rlberry-py/rlberry">github</a>
        </li>
        <!--
        <li class="nav-item dropdown nav-more-item-dropdown">
          <a class="sk-nav-link nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">More</a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdown">
          </div>
        </li>-->
    </ul>
      <div id="searchbox" role="search">
          <div class="searchformwrapper">
            <form action="https://duckduckgo.com/">
            <input type="hidden" id="sites" name="sites" value="https://rlberry-py.github.io/rlberry/">
            <input type="search" placeholder="Search &hellip;" value="" name="q" />
            <input class="sk-search-text-btn" type="submit" value="Go" /></form>

          </div>
      </div>
    </div>
  </div>
</nav>
<div class="d-flex" id="sk-doc-wrapper">
    <input type="checkbox" name="sk-toggle-checkbox" id="sk-toggle-checkbox">
    <label id="sk-sidemenu-toggle" class="sk-btn-toggle-toc btn sk-btn-primary" for="sk-toggle-checkbox">Toggle Menu</label>
    <div id="sk-sidebar-wrapper" class="border-right">
      <div class="sk-sidebar-toc-wrapper">
        <div class="sk-sidebar-toc-logo">
        </div>
	<br>
        <div class="alert alert-warning p-1 mb-2" role="alert">

          <p class="text-center mb-0">
          rlberry v0.6.0.post25.dev0+7b18221 <br/>
          <a href="../../versions.html">Other versions</a>
          </p>

        </div>
            <div class="sk-sidebar-toc">
              <ul>
<li><a class="reference internal" href="#">Quick Start for Reinforcement Learning in rlberry</a><ul>
<li><a class="reference internal" href="#importing-required-libraries">Importing required libraries</a></li>
<li><a class="reference internal" href="#choosing-an-rl-environment">Choosing an RL environment</a></li>
<li><a class="reference internal" href="#defining-an-agent-and-a-baseline">Defining an agent and a baseline</a></li>
<li><a class="reference internal" href="#agent-manager">Agent Manager</a></li>
<li><a class="reference internal" href="#comparing-the-expected-rewards-of-the-final-policies">Comparing the expected rewards of the final policies</a></li>
<li><a class="reference internal" href="#comparing-the-agents-during-the-learning-period">Comparing the agents during the learning period</a></li>
</ul>
</li>
</ul>

            </div>
      </div>
    </div>
    <div id="sk-page-content-wrapper">
      <div class="sk-page-content container-fluid body px-md-3" role="main">
        
  <section id="quick-start-for-reinforcement-learning-in-rlberry">
<h1>Quick Start for Reinforcement Learning in rlberry<a class="headerlink" href="#quick-start-for-reinforcement-learning-in-rlberry" title="Permalink to this heading">¶</a></h1>
<div class="math notranslate nohighlight">
\[\def\CC{\bf C}
\def\QQ{\bf Q}
\def\RR{\bf R}
\def\ZZ{\bf Z}
\def\NN{\bf N}\]</div>
<section id="importing-required-libraries">
<h2>Importing required libraries<a class="headerlink" href="#importing-required-libraries" title="Permalink to this heading">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">rlberry.agents</span> <span class="kn">import</span> <span class="n">UCBVIAgent</span><span class="p">,</span> <span class="n">AgentWithSimplePolicy</span>
<span class="kn">from</span> <span class="nn">rlberry.envs</span> <span class="kn">import</span> <span class="n">Chain</span>
<span class="kn">from</span> <span class="nn">rlberry.manager</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ExperimentManager</span><span class="p">,</span>
    <span class="n">evaluate_agents</span><span class="p">,</span>
    <span class="n">plot_writer_data</span><span class="p">,</span>
    <span class="n">read_writer_data</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">rlberry.wrappers</span> <span class="kn">import</span> <span class="n">WriterWrapper</span>
</pre></div>
</div>
</section>
<section id="choosing-an-rl-environment">
<h2>Choosing an RL environment<a class="headerlink" href="#choosing-an-rl-environment" title="Permalink to this heading">¶</a></h2>
<p>In this tutorial, we will use the <code class="xref py py-class docutils literal notranslate"><span class="pre">Chain</span></code>
environment, which is a very simple environment where the agent has to go from one
end of a chain to the other end.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">env_ctor</span> <span class="o">=</span> <span class="n">Chain</span>
<span class="n">env_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">L</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fail_prob</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="c1"># chain of length 10. With proba 0.2, the agent will not be able to take the action it wants to take/</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">env_ctor</span><span class="p">(</span><span class="o">**</span><span class="n">env_kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p>Let us see a graphical representation</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span><span class="o">.</span><span class="n">enable_rendering</span><span class="p">()</span>
<span class="n">observation</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">for</span> <span class="n">tt</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>
<span class="n">video</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">save_video</span><span class="p">(</span><span class="s2">&quot;video_chain.mp4&quot;</span><span class="p">,</span> <span class="n">framerate</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>ffmpeg version n5.0 Copyright (c) 2000-2022 the FFmpeg developers
  built with gcc 11.2.0 (GCC)
  configuration: --prefix=/usr --disable-debug --disable-static --disable-stripping --enable-amf --enable-avisynth --enable-cuda-llvm --enable-lto --enable-fontconfig --enable-gmp --enable-gnutls --enable-gpl --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libdav1d --enable-libdrm --enable-libfreetype --enable-libfribidi --enable-libgsm --enable-libiec61883 --enable-libjack --enable-libmfx --enable-libmodplug --enable-libmp3lame --enable-libopencore_amrnb --enable-libopencore_amrwb --enable-libopenjpeg --enable-libopus --enable-libpulse --enable-librav1e --enable-librsvg --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtheora --enable-libv4l2 --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxcb --enable-libxml2 --enable-libxvid --enable-libzimg --enable-nvdec --enable-nvenc --enable-shared --enable-version3
  libavutil      57. 17.100 / 57. 17.100
  libavcodec     59. 18.100 / 59. 18.100
  libavformat    59. 16.100 / 59. 16.100
  libavdevice    59.  4.100 / 59.  4.100
  libavfilter     8. 24.100 /  8. 24.100
  libswscale      6.  4.100 /  6.  4.100
  libswresample   4.  3.100 /  4.  3.100
  libpostproc    56.  3.100 / 56.  3.100
Input #0, rawvideo, from &#39;pipe:&#39;:
  Duration: N/A, start: 0.000000, bitrate: 7680 kb/s
  Stream #0:0: Video: rawvideo (RGB[24] / 0x18424752), rgb24, 800x80, 7680 kb/s, 5 tbr, 5 tbn
Stream mapping:
  Stream #0:0 -&gt; #0:0 (rawvideo (native) -&gt; h264 (libx264))
[libx264 @ 0x564b9e570340] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2
[libx264 @ 0x564b9e570340] profile High, level 1.2, 4:2:0, 8-bit
[libx264 @ 0x564b9e570340] 264 - core 164 r3081 19856cc - H.264/MPEG-4 AVC codec - Copyleft 2003-2021 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=2 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=5 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00
Output #0, mp4, to &#39;video_chain.mp4&#39;:
  Metadata:
    encoder         : Lavf59.16.100
  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv420p(tv, progressive), 800x80, q=2-31, 5 fps, 10240 tbn
    Metadata:
      encoder         : Lavc59.18.100 libx264
    Side data:
      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A
frame=    6 fps=0.0 q=-1.0 Lsize=       4kB time=00:00:00.60 bitrate=  48.8kbits/s speed=56.2x
video:3kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 32.212582%
[libx264 @ 0x564b9e570340] frame I:1     Avg QP: 6.94  size:   742
[libx264 @ 0x564b9e570340] frame P:5     Avg QP:22.68  size:   267
[libx264 @ 0x564b9e570340] mb I  I16..4: 95.2%  0.0%  4.8%
[libx264 @ 0x564b9e570340] mb P  I16..4:  1.2%  2.1%  2.0%  P16..4:  0.2%  0.0%  0.0%  0.0%  0.0%    skip:94.6%
[libx264 @ 0x564b9e570340] 8x8 transform intra:8.2% inter:0.0%
[libx264 @ 0x564b9e570340] coded y,uvDC,uvAC intra: 6.5% 12.3% 11.4% inter: 0.0% 0.0% 0.0%
[libx264 @ 0x564b9e570340] i16 v,h,dc,p: 79%  1% 20%  0%
[libx264 @ 0x564b9e570340] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu:  0%  0% 100%  0%  0%  0%  0%  0%  0%
[libx264 @ 0x564b9e570340] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 52% 22% 19%  1%  0%  3%  1%  3%  1%
[libx264 @ 0x564b9e570340] i8c dc,h,v,p: 92%  4%  3%  0%
[libx264 @ 0x564b9e570340] Weighted P-Frames: Y:0.0% UV:0.0%
[libx264 @ 0x564b9e570340] kb/s:13.85
</pre></div>
</div>
<p>The agent has two actions, go to the left of to the right, but it might
move to a random direction according to a failure probability
<code class="docutils literal notranslate"><span class="pre">fail_prob=0.1</span></code>.</p>
</section>
<section id="defining-an-agent-and-a-baseline">
<h2>Defining an agent and a baseline<a class="headerlink" href="#defining-an-agent-and-a-baseline" title="Permalink to this heading">¶</a></h2>
<p>We will compare a RandomAgent (which plays random action) to the
<code class="xref py py-class docutils literal notranslate"><span class="pre">UCBVIAgent</span></code>, which
is a algorithm that is designed to perform an efficient exploration.
Our goal is then to assess the performance of the two algorithms.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create random agent as a baseline</span>
<span class="k">class</span> <span class="nc">RandomAgent</span><span class="p">(</span><span class="n">AgentWithSimplePolicy</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;RandomAgent&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">AgentWithSimplePolicy</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">budget</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">observation</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">ep</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">budget</span><span class="p">):</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
            <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># choose an action at random</span>


<span class="c1"># Define parameters</span>
<span class="n">ucbvi_params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span> <span class="s2">&quot;horizon&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">}</span>
</pre></div>
</div>
<p>There are a number of agents that are already coded in rlberry. See the
module <a class="reference internal" href="../../generated/rlberry.agents.Agent.html#rlberry.agents.Agent" title="rlberry.agents.Agent"><code class="xref py py-class docutils literal notranslate"><span class="pre">Agent</span></code></a> for more informations.</p>
</section>
<section id="agent-manager">
<h2>Agent Manager<a class="headerlink" href="#agent-manager" title="Permalink to this heading">¶</a></h2>
<p>One of the main feature of rlberry is its <a class="reference internal" href="../../generated/rlberry.manager.ExperimentManager.html#rlberry.manager.ExperimentManager" title="rlberry.manager.ExperimentManager"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExperimentManager</span></code></a>
class. Here is a diagram to explain briefly what it does.</p>
<figure class="align-center">
<img alt="basics/quick_start_rl/experiment_manager_diagram.png" src="basics/quick_start_rl/experiment_manager_diagram.png" />
</figure>
<p>In a few words, agent manager spawns agents and environments for training and
then once the agents are trained, it uses these agents and new environments
to evaluate how well the agent perform. All of these steps can be
done several times to assess stochasticity of agents and/or environment.</p>
</section>
<section id="comparing-the-expected-rewards-of-the-final-policies">
<h2>Comparing the expected rewards of the final policies<a class="headerlink" href="#comparing-the-expected-rewards-of-the-final-policies" title="Permalink to this heading">¶</a></h2>
<p>We want to assess the expected reward of the policy learned by our agents
for a time horizon of (say) <span class="math notranslate nohighlight">\(T=20\)</span>.</p>
<p>To do that we use 10 Monte-Carlo simulations, i.e., we do the experiment
10 times for each agent and at the end we take the mean of the 10
obtained reward.</p>
<p>This gives us 1 value per agent. We do this 10 times (so 10 times 10
equal 100 simulations) in order to have an idea of the variability of
our estimation.</p>
<p>In order to manage the agents, we use an Agent Manager. The manager will
then spawn agents as desired during the experiment.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create ExperimentManager to fit 1 agent</span>
<span class="n">ucbvi_stats</span> <span class="o">=</span> <span class="n">ExperimentManager</span><span class="p">(</span>
    <span class="n">UCBVIAgent</span><span class="p">,</span>
    <span class="p">(</span><span class="n">env_ctor</span><span class="p">,</span> <span class="n">env_kwargs</span><span class="p">),</span>
    <span class="n">fit_budget</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">eval_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">eval_horizon</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_simulations</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
    <span class="n">init_kwargs</span><span class="o">=</span><span class="n">ucbvi_params</span><span class="p">,</span>
    <span class="n">n_fit</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ucbvi_stats</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Create ExperimentManager for baseline</span>
<span class="n">baseline_stats</span> <span class="o">=</span> <span class="n">ExperimentManager</span><span class="p">(</span>
    <span class="n">RandomAgent</span><span class="p">,</span>
    <span class="p">(</span><span class="n">env_ctor</span><span class="p">,</span> <span class="n">env_kwargs</span><span class="p">),</span>
    <span class="n">fit_budget</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">eval_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">eval_horizon</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_simulations</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
    <span class="n">n_fit</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">baseline_stats</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[INFO] Running ExperimentManager fit() for UCBVI with n_fit = 1 and max_workers = None.
[INFO] ... trained!
[INFO] Running ExperimentManager fit() for RandomAgent with n_fit = 1 and max_workers = None.
[INFO] ... trained!
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">evaluate_agents</span><span class="p">([</span><span class="n">ucbvi_stats</span><span class="p">,</span> <span class="n">baseline_stats</span><span class="p">],</span> <span class="n">n_simulations</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[INFO] Evaluating UCBVI...
[INFO] [eval]... simulation 1/10
[INFO] [eval]... simulation 2/10
[INFO] [eval]... simulation 3/10
[INFO] [eval]... simulation 4/10
[INFO] [eval]... simulation 5/10
[INFO] [eval]... simulation 6/10
[INFO] [eval]... simulation 7/10
[INFO] [eval]... simulation 8/10
[INFO] [eval]... simulation 9/10
[INFO] [eval]... simulation 10/10
[INFO] Evaluating RandomAgent...
[INFO] [eval]... simulation 1/10
[INFO] [eval]... simulation 2/10
[INFO] [eval]... simulation 3/10
[INFO] [eval]... simulation 4/10
[INFO] [eval]... simulation 5/10
[INFO] [eval]... simulation 6/10
[INFO] [eval]... simulation 7/10
[INFO] [eval]... simulation 8/10
[INFO] [eval]... simulation 9/10
[INFO] [eval]... simulation 10/10
</pre></div>
</div>
<img alt="../../_images/output_10_1.png" class="align-center" src="../../_images/output_10_1.png" />
</section>
<section id="comparing-the-agents-during-the-learning-period">
<h2>Comparing the agents during the learning period<a class="headerlink" href="#comparing-the-agents-during-the-learning-period" title="Permalink to this heading">¶</a></h2>
<p>In the previous section, we compared the performance of the <strong>final</strong> policies learned by
the agents, <strong>after</strong> the learning period.</p>
<p>To compare the performance of the agents <strong>during</strong> the learning period
(in the fit method), we can estimate their cumulative regret, which is the difference
between the rewards gathered by the agents during training and the
rewards of an optimal agent. Alternatively, if the we cannot compute the optimal
policy, we could simply compare the rewards gathered during learning, instead of the regret.</p>
<p>First, we have to record the reward during the fit as this is not done
automatically. To do this, we can use the <a class="reference internal" href="../../generated/rlberry.wrappers.WriterWrapper.html#rlberry.wrappers.WriterWrapper" title="rlberry.wrappers.writer_utils.WriterWrapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">WriterWrapper</span></code></a>
module, or simply the <cite>Agent.writer</cite> attribute.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RandomAgent2</span><span class="p">(</span><span class="n">RandomAgent</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;RandomAgent2&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">RandomAgent</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">WriterWrapper</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="p">,</span> <span class="n">write_scalar</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">UCBVIAgent2</span><span class="p">(</span><span class="n">UCBVIAgent</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;UCBVIAgent2&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">UCBVIAgent</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">WriterWrapper</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="p">,</span> <span class="n">write_scalar</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>To compute the regret, we also need to define an optimal agent. Here
it’s an agent that always chooses the action that moves to the right.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">OptimalAgent</span><span class="p">(</span><span class="n">AgentWithSimplePolicy</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;OptimalAgent&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">AgentWithSimplePolicy</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">WriterWrapper</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="p">,</span> <span class="n">write_scalar</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">budget</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">observation</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">ep</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">budget</span><span class="p">):</span>
            <span class="n">action</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>

    <span class="k">def</span> <span class="nf">policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span>
</pre></div>
</div>
<p>Then, we fit the two agents and plot the data in the writer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create ExperimentManager to fit 4 agents using 1 job</span>
<span class="n">ucbvi_stats</span> <span class="o">=</span> <span class="n">ExperimentManager</span><span class="p">(</span>
    <span class="n">UCBVIAgent2</span><span class="p">,</span>
    <span class="p">(</span><span class="n">env_ctor</span><span class="p">,</span> <span class="n">env_kwargs</span><span class="p">),</span>
    <span class="n">fit_budget</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">init_kwargs</span><span class="o">=</span><span class="n">ucbvi_params</span><span class="p">,</span>
    <span class="n">n_fit</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">parallelization</span><span class="o">=</span><span class="s2">&quot;process&quot;</span><span class="p">,</span>
    <span class="n">mp_context</span><span class="o">=</span><span class="s2">&quot;fork&quot;</span><span class="p">,</span>
<span class="p">)</span>  <span class="c1"># mp_context is needed to have parallel computing in notebooks.</span>
<span class="n">ucbvi_stats</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Create ExperimentManager for baseline</span>
<span class="n">baseline_stats</span> <span class="o">=</span> <span class="n">ExperimentManager</span><span class="p">(</span>
    <span class="n">RandomAgent2</span><span class="p">,</span>
    <span class="p">(</span><span class="n">env_ctor</span><span class="p">,</span> <span class="n">env_kwargs</span><span class="p">),</span>
    <span class="n">fit_budget</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
    <span class="n">n_fit</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">parallelization</span><span class="o">=</span><span class="s2">&quot;process&quot;</span><span class="p">,</span>
    <span class="n">mp_context</span><span class="o">=</span><span class="s2">&quot;fork&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">baseline_stats</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Create ExperimentManager for baseline</span>
<span class="n">opti_stats</span> <span class="o">=</span> <span class="n">ExperimentManager</span><span class="p">(</span>
    <span class="n">OptimalAgent</span><span class="p">,</span>
    <span class="p">(</span><span class="n">env_ctor</span><span class="p">,</span> <span class="n">env_kwargs</span><span class="p">),</span>
    <span class="n">fit_budget</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
    <span class="n">n_fit</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">parallelization</span><span class="o">=</span><span class="s2">&quot;process&quot;</span><span class="p">,</span>
    <span class="n">mp_context</span><span class="o">=</span><span class="s2">&quot;fork&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">opti_stats</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[INFO] Running ExperimentManager fit() for UCBVIAgent2 with n_fit = 10 and max_workers = None.
[INFO] ... trained!
[INFO] Running ExperimentManager fit() for RandomAgent2 with n_fit = 10 and max_workers = None.
[INFO] ... trained!
[INFO] Running ExperimentManager fit() for OptimalAgent with n_fit = 10 and max_workers = None.
[INFO] ... trained!
</pre></div>
</div>
<p>Remark that <code class="docutils literal notranslate"><span class="pre">fit_budget</span></code> may not mean the same thing among agents. For
OptimalAgent and RandomAgent <code class="docutils literal notranslate"><span class="pre">fit_budget</span></code> is the number of steps in
the environments that the agent is allowed to take.</p>
<p>The reward that we recover is recorded every time env.step is called.</p>
<p>For UCBVI this is the number of iterations of the algorithm and in each
iteration, the environment takes 100 steps (<code class="docutils literal notranslate"><span class="pre">horizon</span></code>) times the
<code class="docutils literal notranslate"><span class="pre">fit_budget</span></code>. Hence the fit_budget used here</p>
<p>Next, we estimate the optimal reward using the optimal policy.</p>
<p>Be careful that this is only an estimation: we estimate the optimal
regret using Monte Carlo and the optimal policy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">plot_writer_data</span><span class="p">(</span><span class="n">opti_stats</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;tag&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;reward&quot;</span><span class="p">][[</span><span class="s2">&quot;global_step&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">]]</span>
<span class="n">opti_reward</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;global_step&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()[</span><span class="s2">&quot;value&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
<p>Finally, we plot the cumulative regret using the 5000 reward values.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_regret</span><span class="p">(</span><span class="n">rewards</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">opti_reward</span> <span class="o">-</span> <span class="n">rewards</span><span class="p">[:</span> <span class="nb">len</span><span class="p">(</span><span class="n">opti_reward</span><span class="p">)])</span>


<span class="c1"># Plot of the cumulative reward.</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">plot_writer_data</span><span class="p">(</span>
    <span class="p">[</span><span class="n">ucbvi_stats</span><span class="p">,</span> <span class="n">baseline_stats</span><span class="p">],</span>
    <span class="n">tag</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span>
    <span class="n">preprocess_func</span><span class="o">=</span><span class="n">compute_regret</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Cumulative Regret&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<img alt="../../_images/output_19_0.png" class="align-center" src="../../_images/output_19_0.png" />
</section>
</section>


      </div>
    <div class="container">
      <footer class="sk-content-footer">
            &copy; 2023, rlberry team.
          <a href="../../_sources/basics/quick_start_rl/quickstart.rst.txt" rel="nofollow">Show this page source</a>
      </footer>
    </div>
  </div>
</div>
<script src="../../_static/js/vendor/bootstrap.min.js"></script>

<script>
    window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
    ga('create', 'UA-22606712-2', 'auto');
    ga('set', 'anonymizeIp', true);
    ga('send', 'pageview');
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


<script>
$(document).ready(function() {
    /* Add a [>>>] button on the top-right corner of code samples to hide
     * the >>> and ... prompts and the output and thus make the code
     * copyable. */
    var div = $('.highlight-python .highlight,' +
                '.highlight-python3 .highlight,' +
                '.highlight-pycon .highlight,' +
		'.highlight-default .highlight')
    var pre = div.find('pre');

    // get the styles from the current theme
    pre.parent().parent().css('position', 'relative');
    var hide_text = 'Hide prompts and outputs';
    var show_text = 'Show prompts and outputs';

    // create and add the button to all the code blocks that contain >>>
    div.each(function(index) {
        var jthis = $(this);
        if (jthis.find('.gp').length > 0) {
            var button = $('<span class="copybutton">&gt;&gt;&gt;</span>');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
            jthis.prepend(button);
        }
        // tracebacks (.gt) contain bare text elements that need to be
        // wrapped in a span to work with .nextUntil() (see later)
        jthis.find('pre:has(.gt)').contents().filter(function() {
            return ((this.nodeType == 3) && (this.data.trim().length > 0));
        }).wrap('<span>');
    });

    // define the behavior of the button when it's clicked
    $('.copybutton').click(function(e){
        e.preventDefault();
        var button = $(this);
        if (button.data('hidden') === 'false') {
            // hide the code output
            button.parent().find('.go, .gp, .gt').hide();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'hidden');
            button.css('text-decoration', 'line-through');
            button.attr('title', show_text);
            button.data('hidden', 'true');
        } else {
            // show the code output
            button.parent().find('.go, .gp, .gt').show();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'visible');
            button.css('text-decoration', 'none');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
        }
    });

	/*** Add permalink buttons next to glossary terms ***/
	$('dl.glossary > dt[id]').append(function() {
		return ('<a class="headerlink" href="#' +
			    this.getAttribute('id') +
			    '" title="Permalink to this term">¶</a>');
	});
  /*** Hide navbar when scrolling down ***/
  // Returns true when headerlink target matches hash in url
  (function() {
    hashTargetOnTop = function() {
        var hash = window.location.hash;
        if ( hash.length < 2 ) { return false; }

        var target = document.getElementById( hash.slice(1) );
        if ( target === null ) { return false; }

        var top = target.getBoundingClientRect().top;
        return (top < 2) && (top > -2);
    };

    // Hide navbar on load if hash target is on top
    var navBar = document.getElementById("navbar");
    var navBarToggler = document.getElementById("sk-navbar-toggler");
    var navBarHeightHidden = "-" + navBar.getBoundingClientRect().height + "px";
    var $window = $(window);

    hideNavBar = function() {
        navBar.style.top = navBarHeightHidden;
    };

    showNavBar = function() {
        navBar.style.top = "0";
    }

    if (hashTargetOnTop()) {
        hideNavBar()
    }

    var prevScrollpos = window.pageYOffset;
    hideOnScroll = function(lastScrollTop) {
        if (($window.width() < 768) && (navBarToggler.getAttribute("aria-expanded") === 'true')) {
            return;
        }
        if (lastScrollTop > 2 && (prevScrollpos <= lastScrollTop) || hashTargetOnTop()){
            hideNavBar()
        } else {
            showNavBar()
        }
        prevScrollpos = lastScrollTop;
    };

    /*** high performance scroll event listener***/
    var raf = window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        window.oRequestAnimationFrame;
    var lastScrollTop = $window.scrollTop();

    if (raf) {
        loop();
    }

    function loop() {
        var scrollTop = $window.scrollTop();
        if (lastScrollTop === scrollTop) {
            raf(loop);
            return;
        } else {
            lastScrollTop = scrollTop;
            hideOnScroll(lastScrollTop);
            raf(loop);
        }
    }
  })();
});

</script>
    
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    
</body>
</html>